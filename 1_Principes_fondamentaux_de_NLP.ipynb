{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7gQFbUxOQtb"
      },
      "source": [
        "# Principes fondamentaux de NLP: Tokenization, Lemmatization, Stemming, et Sentence Segmentation\n",
        "\n",
        "\n",
        "Le traitement du langage naturel (NLP) a fait des progrès considérables ces dernières années grâce au succès des [techniques modernes](https://nlpoverview.com/) basées sur l'[apprentissage profond](https://en.wikipedia.org/wiki/Deep_learning). Avec l'augmentation de la popularité du NLP et la disponibilité de différentes formes de données à grande échelle, il est aujourd'hui encore plus impératif de comprendre le fonctionnement interne des techniques et des concepts du NLP, à partir des premiers principes, car ils trouvent leur place dans les usages et les applications du monde réel qui affectent la société dans son ensemble. Il est important de développer des intuitions et d'avoir une bonne compréhension des concepts pour mettre au point des techniques innovantes, améliorer la recherche et créer des technologies d'IA et de PNL sûres et centrées sur l'homme.\n",
        "\n",
        "Dans ce Notebook, nous allons découvrir certains des **concepts de base** les plus importants qui alimentent les techniques NLP utilisées pour la recherche et la création d'applications du monde réel. Certaines de ces techniques comprennent la *lemmatisation*, le *stemmingt*, la *tokénisation* et la *segmentation de la phrase*. Ce sont toutes des techniques importantes pour former des modèles NLP efficaces et performants.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7qsKcOFaH2"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1h0ZNzohff1nUWMerrW50eDxY99ArRJTK)\n",
        "\n",
        "Dans toute tâche NLP typique, l'une des premières étapes est de tokeniser vos morceaux de texte en mots/tokens individuels (processus démontré dans la figure ci-dessus), dont le résultat est utilisé pour créer des vocabulaires qui seront utilisés dans le modèle de langage que vous prévoyez de construire. Il s'agit en fait de l'une des techniques que nous utiliserons le plus tout au long de cette série, mais nous nous en tiendrons ici aux principes de base.\n",
        "\n",
        "Je vous montre ci-dessous un exemple de tokéniseur simple qui ne suit aucune norme. Tout ce qu'il fait, c'est extraire les jetons sur la base d'un séparateur d'espace blanc.\n",
        "\n",
        "Essayez d'exécuter les blocs de code suivants.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7xM8HKqAtf"
      },
      "source": [
        "## required libraries that need to be installed\n",
        "%%capture\n",
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUhMRrhFGfqJ",
        "outputId": "244b14db-a6f7-4c88-eeba-c466dcb19f9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## tokenizing a piecen of text\n",
        "doc = \"I love coding and writing\"\n",
        "for i, w in enumerate(doc.split(\" \")):\n",
        "    print(\"Token \" + str(i) + \": \" + w)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 0: I\n",
            "Token 1: love\n",
            "Token 2: coding\n",
            "Token 3: and\n",
            "Token 4: writing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Med-k0CeG8Ke"
      },
      "source": [
        "Tout ce que fait le code est de séparer la phrase en tokens individuels. Le bloc de code simple ci-dessus fonctionne bien avec le texte que j'ai fourni. Mais généralement, le texte est beaucoup plus bruyant et complexe que l'exemple que j'ai utilisé. Par exemple, si j'ai utilisé le mot \"so-called\", s'agit-il d'un seul mot ou de deux mots ? Pour de tels scénarios, vous pouvez avoir besoin d'approches plus avancées pour la tokenisation. Vous pouvez envisager de supprimer le \"-\" et de diviser le mot en deux jetons ou de le combiner en un seul jeton, mais tout dépend du problème et du domaine sur lesquels vous travaillez. \n",
        "\n",
        "Un autre problème avec notre algorithme simple est qu'il ne peut pas traiter les espaces blancs supplémentaires dans le texte. En outre, comment traiter des villes comme \"New York\" et \"San Francisco\" ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSQwXKwrQAp0"
      },
      "source": [
        "La tokenisation peut également prendre différentes formes. Par exemple, plus récemment, un grand nombre de modèles NLP de pointe tels que [BERT](https://arxiv.org/pdf/1810.04805.pdf) utilisent des tokens de `sous-mots` dans lesquels des combinaisons fréquentes de caractères font également partie du vocabulaire. Cela permet de traiter le problème dit du \"hors vocabulaire\" (OOV). Nous en parlerons dans les prochains chapitres, mais si vous souhaitez en savoir plus, consultez cet [article](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf).\n",
        "\n",
        "Pour démontrer comment réaliser une tokénisation plus fiable, nous allons utiliser [spaCy](https://spacy.io/), qui est une bibliothèque Python impressionnante et robuste pour le traitement du langage naturel. En particulier, nous allons utiliser le tokenizer intégré qui se trouve [here](https://spacy.io/usage/linguistic-features#sbd-custom).\n",
        "\n",
        "Exécutez le bloc de code ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpinv_FjoyVx",
        "outputId": "73a28f06-f5a4-4335-e125-c10fef782194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## import the libraries\n",
        "import spacy\n",
        "## load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "## tokenization\n",
        "doc = nlp(\"This is the so-called lemmatization\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "is\n",
            "the\n",
            "so\n",
            "-\n",
            "called\n",
            "lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout ce que fait le code est de tokeniser le texte en se basant sur un modèle de langage pré-construit.\n",
        "\n",
        "Essayez de mettre un texte différent dans la partie `nlp()` du code ci-dessus. Le tokenizer est assez robuste et il inclut une série de règles intégrées qui traitent les exceptions et les cas spéciaux tels que les tokens qui contiennent des puctuations comme \"`\" et \".\", \"-\", etc. Vous pouvez même ajouter vos propres règles, découvrez comment [ici](https://spacy.io/usage/linguistic-features#special-cases).\n",
        "\n",
        "Dans un prochain chapitre de la série, nous nous pencherons sur la tokénisation et les différents outils existants qui peuvent simplifier et accélérer le processus de tokénisation pour créer des vocabulaires. Parmi les outils que nous explorerons, citons [Keras Tokenizer API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) et [Hugging Face Tokenizer](https://github.com/huggingface/tokenizers).  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bw6S9xPES49F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-F16mbBkVXF"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1_-wxBOU_JebjdG1sxoobKYRCtX3dVF0L)\n",
        "\n",
        "[La lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation) est le processus par lequel nous prenons des éléments individuels d'une phrase et nous essayons de les réduire à leur forme *base*. Le processus qui rend cela possible consiste à disposer d'un vocabulaire et à effectuer une analyse morphologique pour supprimer les terminaisons flexionnelles. Le résultat du processus de lemmatisation (comme le montre la figure ci-dessus) est le *lemma* ou la forme de base du mot. Par exemple, un processus de lemmatisation réduit les désinences \"am\", \"are\" et \"is\" à la forme de base \"be\". Jetez un coup d'œil à la figure ci-dessus pour un exemple complet et essayez de comprendre ce qu'il fait.\n",
        "\n",
        "La lemmatisation est utile pour normaliser le texte pour les tâches de classification de texte ou les moteurs de recherche, et une variété d'autres tâches NLP telles que la [classification des sentiments](https://en.wikipedia.org/wiki/Sentiment_analysis). Elle est particulièrement importante lorsqu'il s'agit de langues complexes comme l'arabe et l'espagnol.\n",
        "\n",
        "Pour montrer comment réaliser la lemmatisation et comment elle fonctionne, nous allons à nouveau utiliser [spaCy](https://spacy.io/). En utilisant la classe spaCy [Lemmatizer](https://spacy.io/api/lemmatizer#_title), nous allons convertir quelques mots en leurs lemmes. \n",
        "\n",
        "Ci-dessous je montre un exemple de comment lemmatiser une phrase en utilisant spaCy. Essayez d'exécuter le bloc de code ci-dessous et examinez les résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5QgWANL3JbD",
        "outputId": "a8425808-df6b-45a7-d7de-2160ce22bd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## import the libraries\n",
        "import spacy\n",
        "\n",
        "## lemmatization\n",
        "doc = nlp(u'I love coding and writing')\n",
        "for word in doc: \n",
        "    print(word.text, \"=>\", word.lemma_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => I\n",
            "love => love\n",
            "coding => code\n",
            "and => and\n",
            "writing => write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 1:** Essayez le code ci-dessus avec différentes phrases et voyez si vous obtenez des résultats inattendus. Essayez également d'ajouter des ponctuations et des espaces supplémentaires qui sont plus courants dans le langage naturel. Que se passe-t-il ?"
      ],
      "metadata": {
        "id": "qqLWrk5Yqe5J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfPOGgYkr3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e460f88f-7c6f-4bc1-8b14-207dd84b66f1"
      },
      "source": [
        "### ENTER CODE HERE\n",
        "\n",
        "###"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => I\n",
            "love => love\n",
            "coding => code\n",
            "and => and\n",
            "    =>    \n",
            ", => ,\n",
            "; => ;\n",
            ": => :\n",
            "! => !\n",
            "writing => write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1XcK3OzdPd2ywO8Y4G6vfjuIFthPce3FH)\n",
        "\n",
        "Stemming n'est qu'une version simplifiée de la lemmatisation, où l'on cherche à supprimer le suffixe à la fin du mot. Lors de l'épellation, nous cherchons à réduire le mot *infléchi* ou *dérivé* à sa forme de base. Jetez un coup d'œil à la figure ci-dessus pour avoir une idée du processus.\n",
        "\n",
        "Les processus de déracinement et de lemmatisation impliquent tous deux une [*analyse morphologique*](https://en.wikipedia.org/wiki/Morphology_(linguistique)) où les racines et les affixes (appelés *morphèmes*) sont extraits et utilisés pour réduire les inflexions à leur forme de base. Par exemple, le mot *cats* a deux morphèmes, *cat* et *s*, le *cat* étant le radical et le *s* l'affixe représentant la pluralité.\n",
        "\n",
        "spaCy ne prend pas en charge l'extraction de la racine. Pour cette partie, nous allons donc utiliser [NLTK](https://www.nltk.org/), une autre fantastique bibliothèque NLP en Python. \n",
        "\n",
        "L'exemple simple ci-dessous montre comment vous pouvez faire le stemming des mots dans un texte. Allez-y et exécutez le code pour voir ce qui se passe."
      ],
      "metadata": {
        "id": "83k_cCospnzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "doc = 'I prefer not to argue'\n",
        "for token in doc.split(\" \"):\n",
        "    print(token, '=>' , stemmer.stem(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVN7si5RpgXY",
        "outputId": "1d605ff1-836f-4283-dbda-ca0c07dd9cf4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => i\n",
            "prefer => prefer\n",
            "not => not\n",
            "to => to\n",
            "argue => argu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:** Essayez d'utiliser différentes phrases dans le code ci-dessus et observez l'effet du stemmer."
      ],
      "metadata": {
        "id": "22l0_R21qIdl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vow0MVZxmQQq"
      },
      "source": [
        "###  ENTER CODE HERE\n",
        "\n",
        "###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjOkmpOn9QGL"
      },
      "source": [
        "## Sentence Segmentation\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1aeHpGNWdnA_VdP0VAu7sO1OGW2eusgHH)\n",
        "\n",
        "Lorsque l'on traite un texte, il arrive souvent que l'on doive le décomposer en phrases individuelles. C'est ce qu'on appelle la segmentation de phrases : le processus d'obtention des phrases individuelles à partir d'un corpus de texte. Les segments obtenus peuvent ensuite être analysés individuellement avec les techniques que nous avons apprises précédemment.\n",
        "\n",
        "Dans la bibliothèque spaCy, nous avons le choix d'utiliser un segmenteur de phrases intégré (formé sur des modèles statistiques) ou de construire votre propre méthode basée sur des règles. En fait, nous allons couvrir quelques exemples pour démontrer la difficulté de ce problème."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za0nOPqPAlph"
      },
      "source": [
        "Ci-dessous, j'ai créé une implémentation naïve d'un algorithme de segmentation de phrases sans utiliser aucune sorte de bibliothèque spéciale. Vous pouvez voir que mon code devient de plus en plus complexe (bugs inclus) au fur et à mesure que je commence à considérer plus de règles. Cette sorte d'approche boostrapped ou basée sur des règles est parfois votre seule option selon la langue avec laquelle vous travaillez ou la disponibilité des ressources linguistiques. \n",
        "\n",
        "Exécutez le code ci-dessous pour appliquer un algorithme simple de segmentation de phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJc-bB8E9PVg",
        "outputId": "73bcdef4-d349-49c5-d1e6-3c0941ac5f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## using a simple rule-based segmenter with native python code\n",
        "text = \"I love coding and programming. I also love sleeping!\"\n",
        "\n",
        "current_position = 0\n",
        "cursor = 0\n",
        "sentences = []\n",
        "for c in text:\n",
        "    if c == \".\" or c == \"!\":\n",
        "        sentences.append(text[current_position:cursor+1])\n",
        "        current_position = cursor + 2\n",
        "    cursor+=1\n",
        "\n",
        "print(sentences)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love coding and programming.', 'I also love sleeping!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNyddCAMmnv6"
      },
      "source": [
        "Notre segmenteur de phrases ne segmente les phrases que lorsqu'il rencontre une limite de phrase qui, dans ce cas, est soit un \".\" soit un \" !\". Ce n'est pas le code le plus propre, mais il montre à quel point la tâche peut devenir difficile lorsque nous sommes confrontés à des textes plus riches qui incluent des caractères spéciaux plus variés. Un problème avec mon code est que je ne suis pas capable de faire la différence entre des abréviations comme Dr. et des nombres comme 0,4. Vous pouvez peut-être créer votre propre expression régulière complexe (nous y reviendrons dans le deuxième chapitre) pour traiter ces cas particuliers, mais cela demande beaucoup de travail et de débogage. Heureusement pour nous, il existe des bibliothèques comme spaCy et NLTK qui nous aident dans ce genre de tâches de prétraitement.\n",
        "\n",
        "Essayons la segmentation de phrase fournie par spaCy. Exécutez le code ci-dessous et examinez les résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_M1vypFBj8Y",
        "outputId": "7f158f50-70d8-46dd-e00d-edb9b1b214c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = nlp(\"I love coding and programming. I also love sleeping!\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love coding and programming.\n",
            "I also love sleeping!\n"
          ]
        }
      ]
    }
  ]
}