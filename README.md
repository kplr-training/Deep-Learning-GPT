#  Deep-Learning-GPT

Ressource :

- [NLP basics](https://github.com/dair-ai/nlp_fundamentals/blob/master/1_nlp_basics_tokenization_segmentation.ipynb) 
- [Calculate dot product attention](https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/C4_W2_lecture_notebook_Attention.ipynb)
- [Anatomy of a transformer](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)
- [implement the transformer decoder from scratch](https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/C4_W2_Assignment_Solution.ipynb)
- [ how multi-head causal attention fits into a GPT-2 transformer decoder](https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/C4_W2_lecture_notebook_Transformer_Decoder.ipynb)
- [Text to Text Transfer from Transformers](https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%203/C4_W3_Assignment_Solution.ipynb)
- [Question answering](https://github.com/nlp-with-transformers/notebooks/blob/main/07_question_answering_v2.ipynb)
- [A Transformer Chatbot Tutorial with TensorFlow 2.0](A Transformer Chatbot Tutorial with TensorFlow 2.0)
